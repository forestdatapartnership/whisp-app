{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in default EE initialization: Please authorize access to your Earth Engine account by running\n",
            "\n",
            "earthengine authenticate\n",
            "\n",
            "in your command line, or ee.Authenticate() in Python, and then retry.\n",
            "Libraries imported successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python313\\Lib\\site-packages\\pandera\\_pandas_deprecated.py:157: FutureWarning: Importing pandas-specific classes and functions from the\n",
            "top-level pandera module will be **removed in a future version of pandera**.\n",
            "If you're using pandera to validate pandas objects, we highly recommend updating\n",
            "your import:\n",
            "\n",
            "```\n",
            "# old import\n",
            "import pandera as pa\n",
            "\n",
            "# new import\n",
            "import pandera.pandas as pa\n",
            "```\n",
            "\n",
            "If you're using pandera to validate objects from other compatible libraries\n",
            "like pyspark or polars, see the supported libraries section of the documentation\n",
            "for more information on how to import pandera:\n",
            "\n",
            "https://pandera.readthedocs.io/en/stable/supported_libraries.html\n",
            "\n",
            "To disable this warning, set the environment variable:\n",
            "\n",
            "```\n",
            "export DISABLE_PANDERA_IMPORT_WARNING=True\n",
            "```\n",
            "\n",
            "  warnings.warn(_future_warning, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import ast\n",
        "import openforis_whisp as whisp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using credential path: d:\\work\\fao\\src\\whisp-app\\src\\python\\../../credentials.json\n",
            "d:\\work\\fao\\src\\whisp-app\\src\\python\\../../credentials.json\n",
            "EE initialized with credentials from: d:\\work\\fao\\src\\whisp-app\\src\\python\\../../credentials.json\n",
            "WHISP initialized successfully\n"
          ]
        }
      ],
      "source": [
        "CREDENTIAL_PATH = os.path.join(os.path.abspath(os.getcwd()), \"../../credentials.json\")\n",
        "print(f\"Using credential path: {CREDENTIAL_PATH}\")\n",
        "\n",
        "whisp.initialize_ee(CREDENTIAL_PATH)\n",
        "print(\"WHISP initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AnalysisOptions class defined\n"
          ]
        }
      ],
      "source": [
        "class AnalysisOptions:\n",
        "    def __init__(self, d: dict | None):\n",
        "        d = d or {}\n",
        "        self.external_id_column = d.get('externalIdColumn')\n",
        "        self.unit_type = d.get('unitType')\n",
        "        nc = d.get('nationalCodes')\n",
        "        self.national_codes = [str(c).lower() for c in nc] if isinstance(nc, list) and nc else None\n",
        "\n",
        "\n",
        "print(\"AnalysisOptions class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File path: d:\\work\\fao\\src\\whisp-app\\src\\python\\../../temp/f6549633-ee9c-4700-abb7-b4f6f41e6302.json\n",
            "Legacy mode: False\n"
          ]
        }
      ],
      "source": [
        "file_name = \"f6549633-ee9c-4700-abb7-b4f6f41e6302.json\"\n",
        "file_path=os.path.join(os.path.abspath(os.getcwd()), \"../../temp/\", file_name)\n",
        "\n",
        "# Set legacy mode if needed\n",
        "legacy_mode = False  # Set to True if you want legacy output format\n",
        "\n",
        "print(f\"File path: {file_path}\")\n",
        "print(f\"Legacy mode: {legacy_mode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis options loaded from file\n",
            "External ID column: user_id\n",
            "Unit type: percent\n",
            "National codes: None\n"
          ]
        }
      ],
      "source": [
        "# Load and parse analysis options from file\n",
        "opts = AnalysisOptions(None)\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r') as f:\n",
        "        payload = json.load(f)\n",
        "        opts = AnalysisOptions(payload.get('analysis_options') if isinstance(payload, dict) else None)\n",
        "    print(\"Analysis options loaded from file\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load analysis options from file: {e}\")\n",
        "    print(\"Using default options\")\n",
        "\n",
        "print(f\"External ID column: {opts.external_id_column}\")\n",
        "print(f\"Unit type: {opts.unit_type}\")\n",
        "print(f\"National codes: {opts.national_codes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame kwargs: {'external_id_column': 'user_id', 'unit_type': 'percent'}\n",
            "An error occurred when trying to set the external_id_column: user_id. Error: The column 'user_id' is missing from 68 out of 68 features in the collection. Available properties in first feature: ['system:index']\n",
            "An error occurred during the conversion from EE to DataFrame: The column 'user_id' is missing from 68 out of 68 features in the collection. Available properties in first feature: ['system:index']\n",
            "Creating schema for national_codes: None\n",
            "[reformat.py | log_missing_columns() | l.267] INFO: All columns from dataframe found in the schema.\n",
            "[reformat.py | log_missing_columns() | l.271] WARNING: The following columns in the schema did not match any columns from the results dataframe: \n",
            "plotId, external_id, Area, Geometry_type, Country, ProducerCountry, Admin_Level_1, Centroid_lon, Centroid_lat, Unit, In_waterbody, EUFO_2020, GLAD_Primary, TMF_undist, GFC_TC_2020, Forest_FDaP, ESA_TC_2020, TMF_plant, Oil_palm_Descals, Oil_palm_FDaP, Coffee_FDaP, Cocoa_FDaP, Cocoa_ETH, Rubber_FDaP, Rubber_RBGE, Soy_Song_2020, TMF_def_2000, TMF_def_2001, TMF_def_2002, TMF_def_2003, TMF_def_2004, TMF_def_2005, TMF_def_2006, TMF_def_2007, TMF_def_2008, TMF_def_2009, TMF_def_2010, TMF_def_2011, TMF_def_2012, TMF_def_2013, TMF_def_2014, TMF_def_2015, TMF_def_2016, TMF_def_2017, TMF_def_2018, TMF_def_2019, TMF_def_2020, TMF_def_2021, TMF_def_2022, TMF_def_2023, TMF_def_2024, TMF_deg_2000, TMF_deg_2001, TMF_deg_2002, TMF_deg_2003, TMF_deg_2004, TMF_deg_2005, TMF_deg_2006, TMF_deg_2007, TMF_deg_2008, TMF_deg_2009, TMF_deg_2010, TMF_deg_2011, TMF_deg_2012, TMF_deg_2013, TMF_deg_2014, TMF_deg_2015, TMF_deg_2016, TMF_deg_2017, TMF_deg_2018, TMF_deg_2019, TMF_deg_2020, TMF_deg_2021, TMF_deg_2022, TMF_deg_2023, TMF_deg_2024, GFC_loss_year_2001, GFC_loss_year_2002, GFC_loss_year_2003, GFC_loss_year_2004, GFC_loss_year_2005, GFC_loss_year_2006, GFC_loss_year_2007, GFC_loss_year_2008, GFC_loss_year_2009, GFC_loss_year_2010, GFC_loss_year_2011, GFC_loss_year_2012, GFC_loss_year_2013, GFC_loss_year_2014, GFC_loss_year_2015, GFC_loss_year_2016, GFC_loss_year_2017, GFC_loss_year_2018, GFC_loss_year_2019, GFC_loss_year_2020, GFC_loss_year_2021, GFC_loss_year_2022, GFC_loss_year_2023, GFC_loss_year_2024, RADD_year_2019, RADD_year_2020, RADD_year_2021, RADD_year_2022, RADD_year_2023, RADD_year_2024, RADD_year_2025, ESA_fire_2001, ESA_fire_2002, ESA_fire_2003, ESA_fire_2004, ESA_fire_2005, ESA_fire_2006, ESA_fire_2007, ESA_fire_2008, ESA_fire_2009, ESA_fire_2010, ESA_fire_2011, ESA_fire_2012, ESA_fire_2013, ESA_fire_2014, ESA_fire_2015, ESA_fire_2016, ESA_fire_2017, ESA_fire_2018, ESA_fire_2019, ESA_fire_2020, MODIS_fire_2000, MODIS_fire_2001, MODIS_fire_2002, MODIS_fire_2003, MODIS_fire_2004, MODIS_fire_2005, MODIS_fire_2006, MODIS_fire_2007, MODIS_fire_2008, MODIS_fire_2009, MODIS_fire_2010, MODIS_fire_2011, MODIS_fire_2012, MODIS_fire_2013, MODIS_fire_2014, MODIS_fire_2015, MODIS_fire_2016, MODIS_fire_2017, MODIS_fire_2018, MODIS_fire_2019, MODIS_fire_2020, MODIS_fire_2021, MODIS_fire_2022, MODIS_fire_2023, MODIS_fire_2024, MODIS_fire_2025, TMF_deg_before_2020, TMF_def_before_2020, GFC_loss_before_2020, ESA_fire_before_2020, MODIS_fire_before_2020, RADD_before_2020, TMF_deg_after_2020, TMF_def_after_2020, GFC_loss_after_2020, MODIS_fire_after_2020, RADD_after_2020, GFT_primary, IFL_2020, European_Primary_Forest, GFT_naturally_regenerating, GFT_planted_plantation, IIASA_planted_plantation, TMF_regrowth_2023, ESRI_2023_TC, GLC_FCS30D_TC_2022, Oil_palm_2023_FDaP, Rubber_2023_FDaP, Coffee_FDaP_2023, Cocoa_2023_FDaP, ESRI_2023_crop, GLC_FCS30D_crop_2022, GFW_logging_before_2020, geo\n",
            "DataFrame shape: (0, 181)\n",
            "DataFrame columns: ['plotId', 'external_id', 'Area', 'Geometry_type', 'Country', 'ProducerCountry', 'Admin_Level_1', 'Centroid_lon', 'Centroid_lat', 'Unit', 'In_waterbody', 'EUFO_2020', 'GLAD_Primary', 'TMF_undist', 'GFC_TC_2020', 'Forest_FDaP', 'ESA_TC_2020', 'TMF_plant', 'Oil_palm_Descals', 'Oil_palm_FDaP', 'Coffee_FDaP', 'Cocoa_FDaP', 'Cocoa_ETH', 'Rubber_FDaP', 'Rubber_RBGE', 'Soy_Song_2020', 'TMF_def_2000', 'TMF_def_2001', 'TMF_def_2002', 'TMF_def_2003', 'TMF_def_2004', 'TMF_def_2005', 'TMF_def_2006', 'TMF_def_2007', 'TMF_def_2008', 'TMF_def_2009', 'TMF_def_2010', 'TMF_def_2011', 'TMF_def_2012', 'TMF_def_2013', 'TMF_def_2014', 'TMF_def_2015', 'TMF_def_2016', 'TMF_def_2017', 'TMF_def_2018', 'TMF_def_2019', 'TMF_def_2020', 'TMF_def_2021', 'TMF_def_2022', 'TMF_def_2023', 'TMF_def_2024', 'TMF_deg_2000', 'TMF_deg_2001', 'TMF_deg_2002', 'TMF_deg_2003', 'TMF_deg_2004', 'TMF_deg_2005', 'TMF_deg_2006', 'TMF_deg_2007', 'TMF_deg_2008', 'TMF_deg_2009', 'TMF_deg_2010', 'TMF_deg_2011', 'TMF_deg_2012', 'TMF_deg_2013', 'TMF_deg_2014', 'TMF_deg_2015', 'TMF_deg_2016', 'TMF_deg_2017', 'TMF_deg_2018', 'TMF_deg_2019', 'TMF_deg_2020', 'TMF_deg_2021', 'TMF_deg_2022', 'TMF_deg_2023', 'TMF_deg_2024', 'GFC_loss_year_2001', 'GFC_loss_year_2002', 'GFC_loss_year_2003', 'GFC_loss_year_2004', 'GFC_loss_year_2005', 'GFC_loss_year_2006', 'GFC_loss_year_2007', 'GFC_loss_year_2008', 'GFC_loss_year_2009', 'GFC_loss_year_2010', 'GFC_loss_year_2011', 'GFC_loss_year_2012', 'GFC_loss_year_2013', 'GFC_loss_year_2014', 'GFC_loss_year_2015', 'GFC_loss_year_2016', 'GFC_loss_year_2017', 'GFC_loss_year_2018', 'GFC_loss_year_2019', 'GFC_loss_year_2020', 'GFC_loss_year_2021', 'GFC_loss_year_2022', 'GFC_loss_year_2023', 'GFC_loss_year_2024', 'RADD_year_2019', 'RADD_year_2020', 'RADD_year_2021', 'RADD_year_2022', 'RADD_year_2023', 'RADD_year_2024', 'RADD_year_2025', 'ESA_fire_2001', 'ESA_fire_2002', 'ESA_fire_2003', 'ESA_fire_2004', 'ESA_fire_2005', 'ESA_fire_2006', 'ESA_fire_2007', 'ESA_fire_2008', 'ESA_fire_2009', 'ESA_fire_2010', 'ESA_fire_2011', 'ESA_fire_2012', 'ESA_fire_2013', 'ESA_fire_2014', 'ESA_fire_2015', 'ESA_fire_2016', 'ESA_fire_2017', 'ESA_fire_2018', 'ESA_fire_2019', 'ESA_fire_2020', 'MODIS_fire_2000', 'MODIS_fire_2001', 'MODIS_fire_2002', 'MODIS_fire_2003', 'MODIS_fire_2004', 'MODIS_fire_2005', 'MODIS_fire_2006', 'MODIS_fire_2007', 'MODIS_fire_2008', 'MODIS_fire_2009', 'MODIS_fire_2010', 'MODIS_fire_2011', 'MODIS_fire_2012', 'MODIS_fire_2013', 'MODIS_fire_2014', 'MODIS_fire_2015', 'MODIS_fire_2016', 'MODIS_fire_2017', 'MODIS_fire_2018', 'MODIS_fire_2019', 'MODIS_fire_2020', 'MODIS_fire_2021', 'MODIS_fire_2022', 'MODIS_fire_2023', 'MODIS_fire_2024', 'MODIS_fire_2025', 'TMF_deg_before_2020', 'TMF_def_before_2020', 'GFC_loss_before_2020', 'ESA_fire_before_2020', 'MODIS_fire_before_2020', 'RADD_before_2020', 'TMF_deg_after_2020', 'TMF_def_after_2020', 'GFC_loss_after_2020', 'MODIS_fire_after_2020', 'RADD_after_2020', 'GFT_primary', 'IFL_2020', 'European_Primary_Forest', 'GFT_naturally_regenerating', 'GFT_planted_plantation', 'IIASA_planted_plantation', 'TMF_regrowth_2023', 'ESRI_2023_TC', 'GLC_FCS30D_TC_2022', 'Oil_palm_2023_FDaP', 'Rubber_2023_FDaP', 'Coffee_FDaP_2023', 'Cocoa_2023_FDaP', 'ESRI_2023_crop', 'GLC_FCS30D_crop_2022', 'GFW_logging_before_2020', 'geo']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plotId</th>\n",
              "      <th>external_id</th>\n",
              "      <th>Area</th>\n",
              "      <th>Geometry_type</th>\n",
              "      <th>Country</th>\n",
              "      <th>ProducerCountry</th>\n",
              "      <th>Admin_Level_1</th>\n",
              "      <th>Centroid_lon</th>\n",
              "      <th>Centroid_lat</th>\n",
              "      <th>Unit</th>\n",
              "      <th>...</th>\n",
              "      <th>ESRI_2023_TC</th>\n",
              "      <th>GLC_FCS30D_TC_2022</th>\n",
              "      <th>Oil_palm_2023_FDaP</th>\n",
              "      <th>Rubber_2023_FDaP</th>\n",
              "      <th>Coffee_FDaP_2023</th>\n",
              "      <th>Cocoa_2023_FDaP</th>\n",
              "      <th>ESRI_2023_crop</th>\n",
              "      <th>GLC_FCS30D_crop_2022</th>\n",
              "      <th>GFW_logging_before_2020</th>\n",
              "      <th>geo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 181 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [plotId, external_id, Area, Geometry_type, Country, ProducerCountry, Admin_Level_1, Centroid_lon, Centroid_lat, Unit, In_waterbody, EUFO_2020, GLAD_Primary, TMF_undist, GFC_TC_2020, Forest_FDaP, ESA_TC_2020, TMF_plant, Oil_palm_Descals, Oil_palm_FDaP, Coffee_FDaP, Cocoa_FDaP, Cocoa_ETH, Rubber_FDaP, Rubber_RBGE, Soy_Song_2020, TMF_def_2000, TMF_def_2001, TMF_def_2002, TMF_def_2003, TMF_def_2004, TMF_def_2005, TMF_def_2006, TMF_def_2007, TMF_def_2008, TMF_def_2009, TMF_def_2010, TMF_def_2011, TMF_def_2012, TMF_def_2013, TMF_def_2014, TMF_def_2015, TMF_def_2016, TMF_def_2017, TMF_def_2018, TMF_def_2019, TMF_def_2020, TMF_def_2021, TMF_def_2022, TMF_def_2023, TMF_def_2024, TMF_deg_2000, TMF_deg_2001, TMF_deg_2002, TMF_deg_2003, TMF_deg_2004, TMF_deg_2005, TMF_deg_2006, TMF_deg_2007, TMF_deg_2008, TMF_deg_2009, TMF_deg_2010, TMF_deg_2011, TMF_deg_2012, TMF_deg_2013, TMF_deg_2014, TMF_deg_2015, TMF_deg_2016, TMF_deg_2017, TMF_deg_2018, TMF_deg_2019, TMF_deg_2020, TMF_deg_2021, TMF_deg_2022, TMF_deg_2023, TMF_deg_2024, GFC_loss_year_2001, GFC_loss_year_2002, GFC_loss_year_2003, GFC_loss_year_2004, GFC_loss_year_2005, GFC_loss_year_2006, GFC_loss_year_2007, GFC_loss_year_2008, GFC_loss_year_2009, GFC_loss_year_2010, GFC_loss_year_2011, GFC_loss_year_2012, GFC_loss_year_2013, GFC_loss_year_2014, GFC_loss_year_2015, GFC_loss_year_2016, GFC_loss_year_2017, GFC_loss_year_2018, GFC_loss_year_2019, GFC_loss_year_2020, GFC_loss_year_2021, GFC_loss_year_2022, GFC_loss_year_2023, GFC_loss_year_2024, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 181 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_kwargs = {}\n",
        "if opts.national_codes: \n",
        "    df_kwargs['national_codes'] = opts.national_codes\n",
        "if opts.external_id_column: \n",
        "    df_kwargs['external_id_column'] = opts.external_id_column\n",
        "if opts.unit_type: \n",
        "    df_kwargs['unit_type'] = opts.unit_type\n",
        "\n",
        "print(f\"DataFrame kwargs: {df_kwargs}\")\n",
        "\n",
        "whisp_df = whisp.whisp_formatted_stats_geojson_to_df(file_path, **df_kwargs)\n",
        "\n",
        "print(f\"DataFrame columns: {list(whisp_df.columns)}\")\n",
        "whisp_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform WHISP risk analysis\n",
        "whisp_df_risk = whisp.whisp_risk(\n",
        "    whisp_df,\n",
        "    explicit_unit_type=opts.unit_type if opts.unit_type else None,\n",
        "    national_codes=opts.national_codes if opts.national_codes else None\n",
        ")\n",
        "\n",
        "print(f\"Risk analysis completed\")\n",
        "print(f\"Result DataFrame shape: {whisp_df_risk.shape}\")\n",
        "print(f\"Result DataFrame columns: {list(whisp_df_risk.columns)}\")\n",
        "whisp_df_risk.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data - handle NaN and infinity values\n",
        "print(\"Cleaning data...\")\n",
        "\n",
        "for col in whisp_df_risk.columns:\n",
        "    if pd.api.types.is_numeric_dtype(whisp_df_risk[col]):\n",
        "        # Replace NaN, infinity values with None for numeric columns\n",
        "        whisp_df_risk[col] = whisp_df_risk[col].replace([np.nan, np.inf, -np.inf], None)\n",
        "    elif whisp_df_risk[col].dtype == 'object':\n",
        "        # Fill NaN values with empty string for object columns\n",
        "        whisp_df_risk[col] = whisp_df_risk[col].fillna('')\n",
        "\n",
        "print(\"Data cleaning completed\")\n",
        "\n",
        "# Check for any remaining problematic values\n",
        "print(\"\\nChecking for remaining NaN/inf values:\")\n",
        "for col in whisp_df_risk.columns:\n",
        "    if pd.api.types.is_numeric_dtype(whisp_df_risk[col]):\n",
        "        nan_count = whisp_df_risk[col].isna().sum()\n",
        "        inf_count = np.isinf(whisp_df_risk[col].fillna(0)).sum()\n",
        "        if nan_count > 0 or inf_count > 0:\n",
        "            print(f\"  {col}: {nan_count} NaN, {inf_count} inf values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results\n",
        "csv_file_path = os.path.splitext(file_path)[0] + '-result.csv'\n",
        "json_file_path = os.path.splitext(file_path)[0] + '-result.json'\n",
        "\n",
        "print(f\"Exporting to CSV: {csv_file_path}\")\n",
        "whisp_df_risk.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
        "print(\"CSV export completed\")\n",
        "\n",
        "print(f\"Exporting to JSON: {json_file_path}\")\n",
        "\n",
        "if not legacy_mode:\n",
        "    # Modern format using whisp converter\n",
        "    whisp.convert_df_to_geojson(whisp_df_risk, json_file_path)\n",
        "    print(\"JSON export completed (modern format)\")\n",
        "else:\n",
        "    # Legacy mode - original format output\n",
        "    print(\"Using legacy mode for JSON export...\")\n",
        "    df_dict = whisp_df_risk.to_dict(orient='records')\n",
        "    \n",
        "    for record in df_dict:\n",
        "        for geo_field in ['geojson', 'geometry']:\n",
        "            if geo_field in record and isinstance(record[geo_field], str):\n",
        "                try:\n",
        "                    record[geo_field] = ast.literal_eval(record[geo_field])\n",
        "                except (ValueError, SyntaxError):\n",
        "                    pass\n",
        "    \n",
        "    class CustomJSONEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            if isinstance(obj, float) and (np.isnan(obj) or np.isinf(obj)):\n",
        "                return None\n",
        "            return super().default(obj)\n",
        "            \n",
        "    def clean_nan_values(item):\n",
        "        if isinstance(item, dict):\n",
        "            return {k: clean_nan_values(v) for k, v in item.items()}\n",
        "        elif isinstance(item, list):\n",
        "            return [clean_nan_values(i) for i in item]\n",
        "        elif isinstance(item, float) and (np.isnan(item) or np.isinf(item)):\n",
        "            return None\n",
        "        else:\n",
        "            return item\n",
        "\n",
        "    clean_dict = clean_nan_values(df_dict)\n",
        "\n",
        "    try:\n",
        "        with open(json_file_path, 'w') as outfile:\n",
        "            json.dump(clean_dict, outfile, indent=4, cls=CustomJSONEncoder)\n",
        "        print(f\"JSON data exported to {json_file_path}\")\n",
        "    except TypeError as e:\n",
        "        print(f\"Error in JSON conversion: {e}\")\n",
        "        json_data = whisp_df_risk.to_json(orient='records', date_format='iso', force_ascii=False)\n",
        "        with open(json_file_path, 'w') as outfile:\n",
        "            outfile.write(json_data)\n",
        "        print(f\"Fallback JSON data exported to {json_file_path}\")\n",
        "\n",
        "print(\"Export completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Testing Instructions\n",
        "\n",
        "To use this notebook for testing:\n",
        "\n",
        "1. **Update the file path** in the 4th cell - replace `\"path/to/your/test/file.json\"` with your actual GeoJSON file path\n",
        "2. **Run each cell sequentially** to step through the analysis process\n",
        "3. **Inspect intermediate results** by examining the DataFrames (`whisp_df`, `whisp_df_risk`)\n",
        "4. **Modify parameters** as needed:\n",
        "   - Change `legacy_mode` to `True` if you need the legacy JSON format\n",
        "   - Modify analysis options manually if needed\n",
        "5. **Check outputs** - CSV and JSON files will be saved with `-result` suffix\n",
        "\n",
        "The notebook breaks down the original script into logical steps, allowing you to:\n",
        "- Test with different input files\n",
        "- Examine intermediate data transformations\n",
        "- Debug issues step by step\n",
        "- Modify parameters without restarting the entire process\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
